>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 â€“ Introduction to Testing and Defect Tracking**

| Group: 15      |
|-----------------|
| Sajan Hayer             |   
| Manraj Singh             |   
| Noor Nawaz               |   
| Ahmad Elshiltawi           |   


**Table of Contents**

[1 Introduction](#Introduction)

[2 High-level description of the exploratory testing plan](#High-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing](#Comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports](#Notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed and team work/effort was divided](#How-the-pair-testing-was-managed-and-team-workeffort-was-divided)

[6 Difficulties encountered, challenges overcome, and lessons learned](#Difficulties-encountered-challenges-overcome-and-lessons-learned)

[7 Comments/feedback on the lab and lab document itself](#Commentsfeedback-on-the-lab-and-lab-document-itself)

# Introduction


In this lab, we were tasked with performing tests on an ATM system. The testing methods used for this lab were Exploratory testing and Manual Scripted Testing. Exploratory testing requires tests to be designed and executed at the same time. By initially generating a high-level test plan, test cases can be generated by following the plan. Following the execution of a test, bugs are reported if the actual outcome does not meet the expected outcome. Exploratory testing encourages exploration of a software system using a practical testing approach. On the other end, Manual Scripted Testing consists of initially generating a test suite, which contains test cases that target the core functionality of the system. The testing script contains test cases and steps for each test that are well documented. After performing the Manual Scripted Testing, we conducted another round of testing using Regression testing. This allowed us to compare the bugs that were present in Version 1.1 of the system, and compare them to original Version 1.0. All bugs were later reported using Jira to produce a defect report.  

# High-level description of the exploratory testing plan

As a group, we decided to approach the exploratory testing by initially outlining the main functions of the ATM simulator. Before we separated within our groups, we brainstormed specific functionalities that we had to verify, such as the accuracy of the deposit and withdrawal, as well as the balance in the accounts. We decided to make a chart that outlined the use case, the function that we were testing, the initial state of the machine, the input, the expected output and the output that actually happened in order to determine if it passed or failed the test. Once we collectively decided on the functionalities that we intended to test, we separated into groups of two so that one member could carry out the tests, and the other member recorded the results. Once a bug was discovered, we would discuss with the other group to ensure that the bug was not due to user error. As each group went further down the list, we discovered more of the edge cases that we made a note to test as well. Some features that we did not test was the user interface and the security of the application.

This testing approach allowed us to test a broad range of features of the application. Initially identifying the use cases of the ATM system allowed us to develop this broad testing plan, where each core functionality was isolated. Instead of performing a specific testing approach, a broad approach allows for major areas to be tested. Specific testing approaches would allow us to determine edge cases, however we had an ideology that testing broad and core functionalities would allow us to discover edge cases during the process.  

# Comparison of exploratory and manual functional testing

As stated previously, we did the exploratory testing by coming together as a group and outlining the functionalities that we decided were integral to the application. As we progressed further into the testing process, we discovered more functionalities that we decided needed to be tested too. The table that we recorded the results into was very similar to the table that was used for the manual functional test. Through manual functional testing, we were not able to find all the bugs that we were able to discover through exploratory testing, since the manual tests were not as thorough as the exploratory test.

During the exploratory tests, we revealed bugs that we were not intentionally aiming to find. By performing a test case as described in the test plan designed initially, we unintentionally discovered bugs. This was not the case however with manual functional testing as the same bugs were repeatedly found. Although Manual Scripted Testing was more efficient and organized when compared with exploratory testing, it did not consider many edge cases as found in exploratory testing. The Exploratory approach allows testers to exhibit freedom in testing, therefore allowing them to consider edge cases that could be missed when doing manual functional testing. 

After this excercise, we learned that the exploratory testing approach allows testers to consider edge cases when testing. After performing a test case, it is easy to think of other test cases that could be sources of failure; having the freedom to design and execute tests consecutively allows for a thorough testing approach. Manual functional testing is more efficient in a team, however, predesigned test cases can allow some bugs to be ignored, and the same bugs will be repeatedly found.

# Notes and discussion of the peer reviews of defect reports

Manraj and Sajan worked on the test cases that involved the Logging, Printing, Canceling and power on/off functionality. Noor and Ahmad worked on the functionality that was involved with the Deposit, Transfer, Withdraw, Pin and Sessions. After each pair completed all test cases in their test suite, bugs were put together and discussed. Upon discussion, we realized that many of the bugs that we found were with the core functionality of the system (eg. withdrawing money). Team members also realized that many additional bugs were found while performing the steps in a test case. 

# How the pair testing was managed and team work/effort was divided 

All group members initially developed a test plan for the exploratory testing and pitched their ideas. After developing the test plan and approach, all group members discussed test paths, which further helped develop test cases. Following this, the group was divided into two pairs (Manraj/Sajan and Noor/Ahmad), and each pair tested the system using the designed test paths. All bugs reported by each pair were joined after the testing. 

All group members worked on the lab report and defect report. 

# Difficulties encountered, challenges overcome, and lessons learned

As all group members had no prior experience with a bug tracking tool such as Jira, setting up the defect reporting environment was a challenge. Despite this challenge, all group members learned by assisting each other. It was also a challenge during some tests to determine whether a bug was an actual issue, or just the design of the software provided. To overcome this, group members discussed test results and made decisions accordingly. 


# Comments/feedback on the lab and lab document itself

The ATM machine software provided was intuitive to use and easy to run. The lab document had a clear direction with the instructions, however a few parts were confusing to understand what was being asked (eg. updating the defect status to resolved - instructions to mark it as resolved were not very clear). Furthermore, a simple tutorial about how to use Jira and properly report bugs properly would have resolved some confusion that all group members had. Aside from that, the lab was fun and educational, and the thorough instructions helped learn the differences between the two testing methods.  

